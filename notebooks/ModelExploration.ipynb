{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d536b93",
   "metadata": {},
   "source": [
    "# Model exploration\n",
    "\n",
    "  \n",
    "\n",
    "Prerequirements to run this notebook:\n",
    "* download all libraries mentioned in `requirements.txt`\n",
    "* download [filtered ParaNMT-detox corpus dataset](https://github.com/skoltech-nlp/detox/releases/download/emnlp2021/filtered_paranmt.zip)\n",
    "* unzip archive via graphical interface (ui.py) or just by hands to a directory `../data/raw/filtered_paranmt/filtered.tsv`\n",
    "\n",
    "Basic information about the dataset\n",
    "* `reference` (str) - First item from the pair\n",
    "* `ref_tox` (float) - toxicity level of reference text\n",
    "* `translation` (str) - second item from the pair - paraphrazed version of the reference\n",
    "* `trn_tox` (float) - toxicity level of translation text\n",
    "* `similarity` (float) - cosine similarity of the texts\n",
    "* `lenght_diff` (float) - relative length difference between texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e22af56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>translation</th>\n",
       "      <th>similarity</th>\n",
       "      <th>lenght_diff</th>\n",
       "      <th>ref_tox</th>\n",
       "      <th>trn_tox</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>If Alkar is flooding her with psychic waste, t...</td>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>0.785171</td>\n",
       "      <td>0.010309</td>\n",
       "      <td>0.014195</td>\n",
       "      <td>0.981983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Now you're getting nasty.</td>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>0.749687</td>\n",
       "      <td>0.071429</td>\n",
       "      <td>0.065473</td>\n",
       "      <td>0.999039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Well, we could spare your life, for one.</td>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>0.919051</td>\n",
       "      <td>0.268293</td>\n",
       "      <td>0.213313</td>\n",
       "      <td>0.985068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ah! Monkey, you've got to snap out of it.</td>\n",
       "      <td>monkey, you have to wake up.</td>\n",
       "      <td>0.664333</td>\n",
       "      <td>0.309524</td>\n",
       "      <td>0.053362</td>\n",
       "      <td>0.994215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I've got orders to put her down.</td>\n",
       "      <td>I have orders to kill her.</td>\n",
       "      <td>0.726639</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.009402</td>\n",
       "      <td>0.999348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           reference  \\\n",
       "0  If Alkar is flooding her with psychic waste, t...   \n",
       "1                          Now you're getting nasty.   \n",
       "2           Well, we could spare your life, for one.   \n",
       "3          Ah! Monkey, you've got to snap out of it.   \n",
       "4                   I've got orders to put her down.   \n",
       "\n",
       "                                         translation  similarity  lenght_diff  \\\n",
       "0  if Alkar floods her with her mental waste, it ...    0.785171     0.010309   \n",
       "1                        you're becoming disgusting.    0.749687     0.071429   \n",
       "2                      well, we can spare your life.    0.919051     0.268293   \n",
       "3                       monkey, you have to wake up.    0.664333     0.309524   \n",
       "4                         I have orders to kill her.    0.726639     0.181818   \n",
       "\n",
       "    ref_tox   trn_tox  \n",
       "0  0.014195  0.981983  \n",
       "1  0.065473  0.999039  \n",
       "2  0.213313  0.985068  \n",
       "3  0.053362  0.994215  \n",
       "4  0.009402  0.999348  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_table('../data/raw/filtered_paranmt/filtered.tsv', index_col=0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b92d15",
   "metadata": {},
   "source": [
    "# Preprocessing step\n",
    "\n",
    "I will use techniques 1.1-1.3 (from `LearningDataset` notebook) for preprocessing. Main idea is to split translation/reference to toxic/non-toxic columns (as they are mixed) and introduce some threasholds on simmilarity and toxicity.\n",
    "\n",
    "\n",
    "1.1 During data preprocessing introduce new features `toxic` and `non-toxic` based on `ref_tox` and `trn_tox` scores.\\\n",
    "1.2 Introduce `toxisity_difference` threashold duting preprocessing.\\\n",
    "1.3 Tune which `simmilarity` value to consider in order to save sence of the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a8bb935",
   "metadata": {},
   "outputs": [],
   "source": [
    "toxisity_difference = 0.75\n",
    "simmilarity_rate = 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63e0ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries passed: 72.61%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tox_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if Alkar floods her with her mental waste, it ...</td>\n",
       "      <td>0.981983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>you're becoming disgusting.</td>\n",
       "      <td>0.999039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well, we can spare your life.</td>\n",
       "      <td>0.985068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>monkey, you have to wake up.</td>\n",
       "      <td>0.994215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I have orders to kill her.</td>\n",
       "      <td>0.999348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tox_score\n",
       "0  if Alkar floods her with her mental waste, it ...   0.981983\n",
       "1                        you're becoming disgusting.   0.999039\n",
       "2                      well, we can spare your life.   0.985068\n",
       "3                       monkey, you have to wake up.   0.994215\n",
       "4                         I have orders to kill her.   0.999348"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split words to toxic & non-toxic based on rate [1.1]\n",
    "tox_queries = []\n",
    "ntox_queries = []\n",
    "\n",
    "for tox_query, ntox_query, sim, len_diff, tox, ntox in df.values:\n",
    "    if tox < ntox:\n",
    "        tox, ntox = ntox, tox\n",
    "        tox_query, ntox_query = ntox_query, tox_query \n",
    "    \n",
    "    # add threasholds on toxisity_difference [1.2] and simmilarity rate [1.3]\n",
    "    if (tox - ntox) >= toxisity_difference and sim >= simmilarity_rate:\n",
    "        tox_queries.append((tox_query, tox))\n",
    "        ntox_queries.append((ntox_query, ntox))\n",
    "\n",
    "print(f'Queries passed: {round(100 * len(tox_queries) / df.shape[0], 2)}%')\n",
    "\n",
    "# convert processed data to dataframes\n",
    "tox = pd.DataFrame(tox_queries, columns=['message', 'tox_score'])\n",
    "non_tox = pd.DataFrame(ntox_queries, columns=['message', 'tox_score'])\n",
    "\n",
    "tox.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a312ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # !pip install spacy\n",
    "# # !python -m spacy download en\n",
    "# # !pip install torchtext==0.10.0\n",
    "\n",
    "# import spacy\n",
    "# from torchtext.vocab import build_vocab_from_iterator\n",
    "# from torchtext.data import Field, BucketIterator\n",
    "\n",
    "# tokenize_fn = spacy.load('en_core_web_sm')\n",
    "# tokenizer = Field(tokenize=tokenize_fn, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "899c87d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "\n",
    "\n",
    "# tokenize_fn = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "def tokenize_dataset(data: pd.DataFrame, spacy_tokenizer: str = 'en_core_web_sm'):\n",
    "    tokenize_fn = spacy.load(spacy_tokenizer)\n",
    "    \n",
    "    tokenized, tox_scores = [], []\n",
    "    for message, tox_score in tqdm(data.values):\n",
    "        tokenized.append([str(token) for token in tokenize_fn(message.lower())])\n",
    "        tox_scores.append(tox_score)\n",
    "    \n",
    "    new_data = pd.DataFrame(columns=['message', 'tox_score'])\n",
    "    new_data.message = tokenized\n",
    "    new_data.tox_score = tox_scores\n",
    "    \n",
    "    return new_data\n",
    "\n",
    "\n",
    "def count_words(data: pd.DataFrame) -> Counter:\n",
    "    data = data.copy()\n",
    "    \n",
    "    counter = Counter()\n",
    "\n",
    "    for tokens in tqdm(data.message):\n",
    "        for token in tokens:\n",
    "            counter[token] += 1\n",
    "    \n",
    "    return counter\n",
    "\n",
    "def delete_low_frequency_inplace(counter: Counter, threshold: int = 10):\n",
    "    import gc\n",
    "    \n",
    "    for (elem, freq) in list(counter.items()):\n",
    "        if freq < threshold:\n",
    "            del counter[elem]\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db385d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(counter: Counter, print_info: bool = True):\n",
    "    PAD_TOKEN = \"<PAD>\"\n",
    "    UNK_TOKEN = \"<UNK>\"\n",
    "    SOS_TOKEN = \"<SOS>\"\n",
    "    EOS_TOKEN = \"<EOS>\"\n",
    "\n",
    "    SPECIALS = [PAD_TOKEN, UNK_TOKEN, SOS_TOKEN, EOS_TOKEN]\n",
    "\n",
    "    tox_vocab = vocab(counter, specials=SPECIALS)\n",
    "\n",
    "    tox_vocab.set_default_index(tox_vocab[UNK_TOKEN])\n",
    "\n",
    "    if print_info:\n",
    "        print(\"Number of tokens: {}\".format(len(tox_vocab)))\n",
    "    \n",
    "    return tox_vocab\n",
    "\n",
    "\n",
    "def save_vocab(vocab, filename: str = None):\n",
    "    if filename is None:\n",
    "        filename = f'spacy-processed-vocab-{len(vocab)}.pth'\n",
    "    torch.save(vocab, filename)\n",
    "    print(f'Succesfully saved vocab as `{filename}`')  # TODO: save to data/preprocessed folder\n",
    "\n",
    "\n",
    "def load_vocab(filename: str):\n",
    "    return torch.load(filename)\n",
    "\n",
    "\n",
    "# tokenized_tox = tokenize_dataset(tox)\n",
    "# tokenized_tox.to_csv('tokenized-tox.csv')\n",
    "# tokenized_non_tox = tokenize_dataset(non_tox)\n",
    "# tokenized_non_tox.to_csv('tokenized-non-tox.csv')\n",
    "\n",
    "\n",
    "\n",
    "# whole_dataset = pd.concat([tox, non_tox])\n",
    "\n",
    "# word_counter = tokenize_and_count(whole_dataset)\n",
    "# delete_low_frequency_inplace(word_counter, threshold=10)\n",
    "# word_vocab = create_vocab(word_counter)\n",
    "# save_vocab(word_vocab, 'full-words-vocab-10.pth')\n",
    "\n",
    "# del tox_vocab\n",
    "# import gc\n",
    "# gc.collect()\n",
    "\n",
    "# non_tox_counter = tokenize_and_count(non_tox)\n",
    "# delete_low_frequency_inplace(non_tox_counter, threshold=50)\n",
    "# non_tox_vocab = create_vocab(non_tox_counter)\n",
    "# save_vocab(non_tox_vocab, 'non-tox-vocab.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132428e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "tokenized_tox = pd.read_csv('tokenized-tox.csv', index_col=0)\n",
    "tokenized_non_tox = pd.read_csv('tokenized-non-tox.csv', index_col=0)\n",
    "\n",
    "tokenized_tox.message = tokenized_tox.message.apply(ast.literal_eval)\n",
    "tokenized_non_tox.message = tokenized_non_tox.message.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1528cf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 838990/838990 [00:02<00:00, 378257.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 90026\n",
      "Number of words after removing low frequency: 8243\n",
      "Number of tokens: 8247\n",
      "Succesfully saved vocab as `full-words-vocab-40.pth`\n"
     ]
    }
   ],
   "source": [
    "whole_dataset = pd.concat([tokenized_tox, tokenized_non_tox])\n",
    "\n",
    "word_counter = count_words(whole_dataset)\n",
    "\n",
    "print('Number of unique words:', len(word_counter))\n",
    "threshold = 40  # 50 works fine with 7121 words!\n",
    "delete_low_frequency_inplace(word_counter, threshold=threshold)\n",
    "print('Number of words after removing low frequency:', len(word_counter))\n",
    "\n",
    "word_vocab = create_vocab(word_counter)\n",
    "save_vocab(word_vocab, f'full-words-vocab-{threshold}.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8debbd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 345600\n",
      "Test size: 38400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = tokenized_tox\n",
    "y = tokenized_non_tox\n",
    "\n",
    "# split data to train/test\n",
    "dataset_lim = 16000 * 20 # 1 minute is approximately 16k, so let's make epoch approximately 30 minutes *24\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[:dataset_lim], y[:dataset_lim], test_size=0.1, random_state=42)\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print('Train size:', X_train.shape[0])\n",
    "print('Test size:', X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d649f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3846a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tox_vocab = load_vocab('tox-vocab.pth')\n",
    "# non_tox_vocab = load_vocab('non-tox-vocab.pth')\n",
    "\n",
    "vocab = load_vocab(f'full-words-vocab-{threshold}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd635105",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab.vocab import Vocab\n",
    "\n",
    "\n",
    "class ToxicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, \n",
    "                 tokenized_tox_data: pd.DataFrame, \n",
    "                 tokenized_non_tox_data: pd.DataFrame,\n",
    "                 vocab: Vocab,\n",
    "                 max_size: int = 150\n",
    "                ):\n",
    "        self.max_size = max_size\n",
    "        self.tox_data = tokenized_tox_data\n",
    "        self.non_tox_data = tokenized_non_tox_data\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def _get_sentence(self, index: int, is_toxic: bool) -> list[int]:\n",
    "        # retrieves sentence from dataset by index\n",
    "        if is_toxic:\n",
    "            sent = ['<SOS>'] + self.tox_data.message[index] + ['<EOS>']\n",
    "        else:\n",
    "            sent = ['<SOS>'] + self.non_tox_data.message[index] + ['<EOS>']\n",
    "        \n",
    "        if len(sent) <= self.max_size:\n",
    "            sent.extend(['<PAD>'] * (self.max_size - len(sent)))\n",
    "        else:\n",
    "            sent = sent[:self.max_size - 1] + ['<EOS>']\n",
    "        \n",
    "        return self.vocab(sent)\n",
    "    \n",
    "    def __getitem__(self, index) -> tuple[list[int], list[int]]:\n",
    "        return self._get_sentence(index, is_toxic=True), self._get_sentence(index, is_toxic=False)\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.tox_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "626bfc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 75  # 50 worked fine!\n",
    "\n",
    "train_dataset = ToxicDataset(X_train, y_train, vocab, max_size=max_size)\n",
    "test_dataset = ToxicDataset(X_test, y_test, vocab, max_size=max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dd767ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128  # 64 worked fine\n",
    "\n",
    "\n",
    "def collate_batch(batch: list):\n",
    "    toxic, non_toxic = [], []\n",
    "    for (tox, non_tox) in batch:\n",
    "        toxic.append(torch.tensor(tox))\n",
    "        non_toxic.append(torch.tensor(non_tox))\n",
    "    return torch.stack(toxic), torch.stack(non_toxic)\n",
    "\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df87523",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46eb7626",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db457d88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c01134",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84756422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "746a8d18",
   "metadata": {},
   "source": [
    "# Model itself \n",
    "\n",
    "Code structure was taken from [pytorch seq2seq tutorial](https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb) with some modifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f900894b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional = True)\n",
    "        \n",
    "        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        \n",
    "        #embedded = [src len, batch size, emb dim]\n",
    "        \n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "                \n",
    "        #outputs = [src len, batch size, hid dim * num directions]\n",
    "        #hidden = [n layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #hidden is stacked [forward_1, backward_1, forward_2, backward_2, ...]\n",
    "        #outputs are always from the last layer\n",
    "        \n",
    "        #hidden [-2, :, : ] is the last of the forwards RNN \n",
    "        #hidden [-1, :, : ] is the last of the backwards RNN\n",
    "        \n",
    "        #initial decoder hidden is final hidden state of the forwards and backwards \n",
    "        #  encoder RNNs fed through a linear layer\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)))\n",
    "        \n",
    "        #outputs = [src len, batch size, enc hid dim * 2]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        \n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        \n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[1]\n",
    "        src_len = encoder_outputs.shape[0]\n",
    "        \n",
    "        #repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #hidden = [batch size, src len, dec hid dim]\n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim = 2))) \n",
    "        \n",
    "        #energy = [batch size, src len, dec hid dim]\n",
    "\n",
    "        attention = self.v(energy).squeeze(2)\n",
    "        \n",
    "        #attention= [batch size, src len]\n",
    "        \n",
    "        return F.softmax(attention, dim=1)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n",
    "        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "             \n",
    "        #input = [batch size]\n",
    "        #hidden = [batch size, dec hid dim]\n",
    "        #encoder_outputs = [src len, batch size, enc hid dim * 2]\n",
    "        \n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        #input = [1, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        \n",
    "        #embedded = [1, batch size, emb dim]\n",
    "        \n",
    "        a = self.attention(hidden, encoder_outputs)\n",
    "                \n",
    "        #a = [batch size, src len]\n",
    "        \n",
    "        a = a.unsqueeze(1)\n",
    "        \n",
    "        #a = [batch size, 1, src len]\n",
    "        \n",
    "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
    "        \n",
    "        #encoder_outputs = [batch size, src len, enc hid dim * 2]\n",
    "        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "        \n",
    "        #weighted = [batch size, 1, enc hid dim * 2]\n",
    "        \n",
    "        weighted = weighted.permute(1, 0, 2)\n",
    "        \n",
    "        #weighted = [1, batch size, enc hid dim * 2]\n",
    "        \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)\n",
    "        \n",
    "        #rnn_input = [1, batch size, (enc hid dim * 2) + emb dim]\n",
    "            \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))\n",
    "        \n",
    "        #output = [seq len, batch size, dec hid dim * n directions]\n",
    "        #hidden = [n layers * n directions, batch size, dec hid dim]\n",
    "        \n",
    "        #seq len, n layers and n directions will always be 1 in this decoder, therefore:\n",
    "        #output = [1, batch size, dec hid dim]\n",
    "        #hidden = [1, batch size, dec hid dim]\n",
    "        #this also means that output == hidden\n",
    "        assert (output == hidden).all()\n",
    "        \n",
    "        embedded = embedded.squeeze(0)\n",
    "        output = output.squeeze(0)\n",
    "        weighted = weighted.squeeze(0)\n",
    "        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))\n",
    "        \n",
    "        #prediction = [batch size, output dim]\n",
    "        \n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a15663d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DSkBart(nn.Module):\n",
    "    def __init__(self, vocab_size: int, device: str = 'cpu'):\n",
    "        super().__init__()\n",
    "        \n",
    "        ENC_EMB_DIM = 128\n",
    "        DEC_EMB_DIM = 128\n",
    "        ENC_HID_DIM = 256\n",
    "        DEC_HID_DIM = 256\n",
    "        ENC_DROPOUT = 0.5\n",
    "        DEC_DROPOUT = 0.5\n",
    "\n",
    "        self.attention = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "        self.encoder = Encoder(vocab_size, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "        self.decoder = Decoder(vocab_size, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, self.attention)\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \n",
    "        #src = [src len, batch size]\n",
    "        #trg = [trg len, batch size]\n",
    "        #teacher_forcing_ratio is probability to use teacher forcing\n",
    "        #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n",
    "        \n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        \n",
    "        #tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "        \n",
    "        #encoder_outputs is all hidden states of the input sequence, back and forwards\n",
    "        #hidden is the final forward and backward hidden states, passed through a linear layer\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "                \n",
    "        #first input to the decoder is the <sos> tokens\n",
    "        input = trg[0,:]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            \n",
    "            #insert input token embedding, previous hidden state and all encoder hidden states\n",
    "            #receive output tensor (predictions) and new hidden state\n",
    "            output, hidden = self.decoder(input, hidden, encoder_outputs)\n",
    "            \n",
    "            #place predictions in a tensor holding predictions for each token\n",
    "            outputs[t] = output\n",
    "            \n",
    "            #decide if we are going to use teacher forcing or not\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            #get the highest predicted token from our predictions\n",
    "            top1 = output.argmax(1) \n",
    "            \n",
    "            #if teacher forcing, use actual next token as next input\n",
    "            #if not, use predicted token\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c03f432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 11,119,799 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DSkBart(len(vocab), device).to(device)\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd37dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e202e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSkBart(\n",
       "  (attention): Attention(\n",
       "    (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8247, 128)\n",
       "    (rnn): GRU(128, 256, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(8247, 128)\n",
       "    (rnn): GRU(640, 256)\n",
       "    (fc_out): Linear(in_features=896, out_features=8247, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "            \n",
    "model.apply(init_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ab60e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DSkBart(\n",
       "  (attention): Attention(\n",
       "    (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "    (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "  )\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(8247, 128)\n",
       "    (rnn): GRU(128, 256, bidirectional=True)\n",
       "    (fc): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn): Linear(in_features=768, out_features=256, bias=True)\n",
       "      (v): Linear(in_features=256, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(8247, 128)\n",
       "    (rnn): GRU(640, 256)\n",
       "    (fc_out): Linear(in_features=896, out_features=8247, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0010a762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "PAD_IDX = vocab['<PAD>']\n",
    "\n",
    "optimizer = Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7a862dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(tqdm(iterator)):\n",
    "        tox, non_tox = batch\n",
    "        tox, non_tox = tox.to(device), non_tox.to(device)\n",
    "        tox, non_tox = tox.T, non_tox.T\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(tox, non_tox)\n",
    "        \n",
    "        #trg = [trg len, batch size]\n",
    "        #output = [trg len, batch size, output dim]\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        non_tox = non_tox[1:].flatten()\n",
    "        \n",
    "        #trg = [(trg len - 1) * batch size]\n",
    "        #output = [(trg len - 1) * batch size, output dim]\n",
    "        \n",
    "        loss = criterion(output, non_tox)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aba8b725",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(iterator)):\n",
    "            tox, non_tox = batch\n",
    "            tox, non_tox = tox.to(device), non_tox.to(device)\n",
    "            tox, non_tox = tox.T, non_tox.T\n",
    "\n",
    "            output = model(tox, non_tox, 0) #turn off teacher forcing\n",
    "\n",
    "            #trg = [trg len, batch size]\n",
    "            #output = [trg len, batch size, output dim]\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            \n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            non_tox = non_tox[1:].flatten()\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "            loss = criterion(output, non_tox)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96bb0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [30:13<00:00,  1.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:29<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss: 4.306 | Train PPL:  74.171\n",
      "\t Val. Loss: 4.276 |  Val. PPL:  71.970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [30:55<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:30<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02\n",
      "\tTrain Loss: 3.274 | Train PPL:  26.420\n",
      "\t Val. Loss: 4.030 |  Val. PPL:  56.261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:03<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:30<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03\n",
      "\tTrain Loss: 3.008 | Train PPL:  20.245\n",
      "\t Val. Loss: 4.036 |  Val. PPL:  56.584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:13<00:00,  1.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:31<00:00,  3.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04\n",
      "\tTrain Loss: 2.883 | Train PPL:  17.863\n",
      "\t Val. Loss: 3.999 |  Val. PPL:  54.523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:13<00:00,  1.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:32<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05\n",
      "\tTrain Loss: 2.799 | Train PPL:  16.432\n",
      "\t Val. Loss: 3.973 |  Val. PPL:  53.158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:07<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:32<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06\n",
      "\tTrain Loss: 2.741 | Train PPL:  15.508\n",
      "\t Val. Loss: 3.941 |  Val. PPL:  51.491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:09<00:00,  1.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:30<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 07\n",
      "\tTrain Loss: 2.700 | Train PPL:  14.881\n",
      "\t Val. Loss: 3.926 |  Val. PPL:  50.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:00<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:29<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 08\n",
      "\tTrain Loss: 2.661 | Train PPL:  14.311\n",
      "\t Val. Loss: 3.988 |  Val. PPL:  53.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:02<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:31<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 09\n",
      "\tTrain Loss: 2.630 | Train PPL:  13.879\n",
      "\t Val. Loss: 3.985 |  Val. PPL:  53.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:06<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:30<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10\n",
      "\tTrain Loss: 2.605 | Train PPL:  13.527\n",
      "\t Val. Loss: 3.929 |  Val. PPL:  50.836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:02<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:30<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11\n",
      "\tTrain Loss: 2.585 | Train PPL:  13.269\n",
      "\t Val. Loss: 3.989 |  Val. PPL:  53.987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:32<00:00,  1.43it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:33<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12\n",
      "\tTrain Loss: 2.562 | Train PPL:  12.960\n",
      "\t Val. Loss: 4.002 |  Val. PPL:  54.680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:10<00:00,  1.44it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:31<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13\n",
      "\tTrain Loss: 2.550 | Train PPL:  12.807\n",
      "\t Val. Loss: 3.965 |  Val. PPL:  52.712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2700/2700 [31:04<00:00,  1.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 300/300 [01:31<00:00,  3.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14\n",
      "\tTrain Loss: 2.537 | Train PPL:  12.637\n",
      "\t Val. Loss: 3.992 |  Val. PPL:  54.142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "N_EPOCHS = 14\n",
    "CLIP = 2\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, test_dataloader, criterion)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0910aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = torch.load('best-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9dd21e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text: pd.DataFrame, vocab, max_size: int):\n",
    "    tokenized_text = tokenize_dataset(text)\n",
    "    text_dataset = ToxicDataset(tokenized_text, tokenized_text, vocab, max_size=max_size)\n",
    "    text_dataloader = torch.utils.data.DataLoader(dataset=text_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    words = []\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(tqdm(text_dataloader)):\n",
    "            tox, non_tox = batch\n",
    "            tox, non_tox = tox.to(device), non_tox.to(device)\n",
    "            tox, non_tox = tox.T, non_tox.T\n",
    "\n",
    "            output = model(tox, non_tox, 0) #turn off teacher forcing\n",
    "            \n",
    "            words.extend(output.argmax(dim=2).T.cpu().detach().tolist())\n",
    "    \n",
    "    return words\n",
    "\n",
    "#             output_dim = output.shape[-1]\n",
    "            \n",
    "#             output = output[1:].view(-1, output_dim)\n",
    "#             non_tox = non_tox[1:].flatten()\n",
    "\n",
    "            #trg = [(trg len - 1) * batch size]\n",
    "            #output = [(trg len - 1) * batch size, output dim]\n",
    "\n",
    "#             loss = criterion(output, non_tox)\n",
    "\n",
    "#             epoch_loss += loss.item()\n",
    "        \n",
    "#     return epoch_loss / len(iterator)\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fdccc8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>tox_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I hate you, bitch</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i love you</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Now you're getting nasty</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is dumb</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This department of education is so fucking bad...</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  tox_score\n",
       "0                                  I hate you, bitch        0.5\n",
       "1                                         i love you        0.5\n",
       "2                           Now you're getting nasty        0.5\n",
       "3                                       This is dumb        0.5\n",
       "4  This department of education is so fucking bad...        0.5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_df = pd.DataFrame(columns=['message', 'tox_score'])\n",
    "text_df.message = ['I hate you, bitch', 'i love you', \"Now you're getting nasty\", 'This is dumb', 'This department of education is so fucking bad. I hate them, they are all devils who just suck cocks.']\n",
    "text_df.tox_score = 0.5\n",
    "\n",
    "\n",
    "text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8a902346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00, 178.59it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  4.05it/s]\n"
     ]
    }
   ],
   "source": [
    "text_vocab_ids = inference(model, text_df, vocab, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4388210f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_vocab(text: list[list[int]], vocab):\n",
    "    for line in text:\n",
    "        yield vocab.lookup_tokens(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1d2e2426",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['<PAD>',\n",
       "  'i',\n",
       "  'hate',\n",
       "  'you',\n",
       "  ',',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>'],\n",
       " ['<PAD>',\n",
       "  'i',\n",
       "  'love',\n",
       "  'you',\n",
       "  '.',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>'],\n",
       " ['<PAD>',\n",
       "  'now',\n",
       "  'you',\n",
       "  \"'re\",\n",
       "  'getting',\n",
       "  'bad',\n",
       "  '.',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>'],\n",
       " ['<PAD>',\n",
       "  'this',\n",
       "  'is',\n",
       "  'crazy',\n",
       "  '.',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>'],\n",
       " ['<PAD>',\n",
       "  'this',\n",
       "  'whole',\n",
       "  'education',\n",
       "  'of',\n",
       "  'education',\n",
       "  'is',\n",
       "  'so',\n",
       "  'bad',\n",
       "  ',',\n",
       "  'i',\n",
       "  'hate',\n",
       "  'them',\n",
       "  ',',\n",
       "  'they',\n",
       "  'are',\n",
       "  'just',\n",
       "  'the',\n",
       "  'birds',\n",
       "  '.',\n",
       "  '<EOS>',\n",
       "  'birds',\n",
       "  '.',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  'birds',\n",
       "  '.',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>',\n",
       "  '<EOS>']]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(from_vocab(text_vocab_ids, vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28087d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
